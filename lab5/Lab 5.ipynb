{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d37f737f",
   "metadata": {},
   "source": [
    "# Lab 5: Leveraging Open Data from Wikipedia for LLM Prompt Engineering\n",
    "\n",
    "## Overview\n",
    "This lab demonstrates how to extract structured data from Wikipedia pages and use it to create effective prompts for Large Language Models (LLMs). You'll learn to work with real-world financial data, process it programmatically, and engineer prompts for various AI tasks.\n",
    "\n",
    "## Learning Objectives\n",
    "- ✓ Extract financial index components from Wikipedia\n",
    "- ✓ Retrieve company infobox data programmatically\n",
    "- ✓ Build structured datasets from semi-structured web data\n",
    "- ✓ Design effective LLM prompts for different tasks\n",
    "- ✓ Process and clean text data for AI consumption\n",
    "- ✓ Create reusable prompt templates and utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9e1c61",
   "metadata": {},
   "source": [
    "## Part 1: Data Extraction from Wikipedia\n",
    "\n",
    "### What is a Financial Index?\n",
    "A financial index is a composite measure of a subset of companies in a specific market or sector. Examples include:\n",
    "- **S&P 500**: 500 largest US companies\n",
    "- **EURO STOXX 50**: 50 largest Eurozone companies\n",
    "- **DAX**: 40 largest German companies\n",
    "\n",
    "### Your Task\n",
    "1. **Identify components**: Extract the list of companies in each index from Wikipedia\n",
    "2. **Gather company data**: Retrieve detailed information (infoboxes) from each company's Wikipedia page\n",
    "3. **Build a dataset**: Combine all data into structured format suitable for LLM processing\n",
    "4. **Engineer prompts**: Create effective prompts that leverage this data for AI tasks\n",
    "\n",
    "### Data Sources\n",
    "- **Index components**: Wikipedia articles listing index members\n",
    "- **Company data**: Wikipedia infoboxes (structured data boxes on company pages)\n",
    "- **Dump file**: Optional - for advanced analysis of full Wikipedia articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e42bbe",
   "metadata": {},
   "source": [
    "### Optional: Full Wikipedia Dump\n",
    "For advanced analysis, you can download the complete Wikipedia dump from:\n",
    "- **Link**: https://dumps.wikimedia.org/enwiki/\n",
    "- **File**: `enwiki-latest-pages-articles-multistream-index.txt.bz2`\n",
    "- **Use case**: Full-text search, article history analysis, or complete data scraping\n",
    "- **Note**: Very large files (100+ GB) - requires significant storage and processing power\n",
    "\n",
    "For this lab, we'll focus on extracting specific data via the Wikipedia API, which is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02aa961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS & SETUP\n",
    "# ============================================================================\n",
    "# These libraries enable us to work with Wikipedia data\n",
    "\n",
    "import pandas as pd              # Data manipulation and analysis\n",
    "import urllib.request           # HTTP requests to Wikipedia\n",
    "from pathlib import Path        # Cross-platform file path handling\n",
    "from typing import Union        # Type hints for better code clarity\n",
    "from tqdm import tqdm          # Progress bars for long operations\n",
    "import wptools               # Wikipedia parsing (infobox extraction)\n",
    "from loguru import logger       # Enhanced logging\n",
    "import json                     # Working with JSON data\n",
    "import numpy as np             # Numerical operations\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25c9b8",
   "metadata": {},
   "source": [
    "## Step 1: Extract Index Components from Wikipedia\n",
    "\n",
    "### Task: Extract Company Lists\n",
    "We'll extract the list of companies that make up each financial index directly from Wikipedia.\n",
    "\n",
    "### Indices We're Covering:\n",
    "1. **S&P 500** (USA) - 500 largest US companies\n",
    "2. **EURO STOXX 50** (Europe) - 50 largest Eurozone companies  \n",
    "3. **CAC 40** (France) - 40 largest French companies\n",
    "4. **DAX** (Germany) - 40 largest German companies\n",
    "5. **CSI 300** (China) - 300 largest Chinese companies\n",
    "6. **S&P Latin America 40** (Latin America) - 40 major LA companies\n",
    "7. **BSE SENSEX** (India) - 30 largest Indian companies\n",
    "8. **NASDAQ-100** (USA Tech) - 100 largest non-financial NASDAQ companies\n",
    "\n",
    "### How It Works:\n",
    "- Each index has a Wikipedia article with a table listing its components\n",
    "- We'll use `pd.read_html()` to extract all tables from these pages\n",
    "- Tables are saved as CSV files for later processing\n",
    "- This approach is fast, requires no authentication, and respects Wikipedia's terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1022b92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FUNCTION 1: Extract Tables from Wikipedia\n",
    "# ============================================================================\n",
    "# This function downloads tables from Wikipedia articles and saves them locally\n",
    "\n",
    "def get_index_components(wiki_url: str, save_dir: Union[str, Path], \n",
    "                         opener: urllib.request.OpenerDirector) -> None:\n",
    "    \"\"\"\n",
    "    Extract all HTML tables from a Wikipedia page and save as CSV files.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    wiki_url : str\n",
    "        The Wikipedia page URL to scrape (e.g., list of index components)\n",
    "    save_dir : Union[str, Path]\n",
    "        Directory where CSV files will be saved\n",
    "    opener : urllib.request.OpenerDirector\n",
    "        Custom URL opener with proper User-Agent headers\n",
    "        \n",
    "    Output:\n",
    "    -------\n",
    "    Creates CSV files named table_0.csv, table_1.csv, etc. in save_dir\n",
    "    Each file contains one table from the Wikipedia page\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> get_index_components(\n",
    "    ...     \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
    "    ...     \"./data/indices/sp500\",\n",
    "    ...     opener\n",
    "    ... )\n",
    "    \"\"\"\n",
    "    # Create the save directory if it doesn't exist\n",
    "    save_path = Path(save_dir)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Read all HTML tables from the Wikipedia page\n",
    "        tables = pd.read_html(wiki_url)\n",
    "        \n",
    "        # Save each table as a separate CSV file\n",
    "        for idx, table in enumerate(tables):\n",
    "            csv_path = save_path / f\"table_{idx}.csv\"\n",
    "            table.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            logger.info(f\"Saved table {idx} to {csv_path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error extracting tables from {wiki_url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b28fd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Configure Wikipedia Index URLs and HTTP Headers\n",
    "# ============================================================================\n",
    "\n",
    "# Dictionary mapping index names to their Wikipedia article URLs\n",
    "# These URLs contain tables with the company components of each index\n",
    "indices = {\n",
    "    \"sp500\": \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\",\n",
    "    \"eurostoxx50\": \"https://en.wikipedia.org/wiki/EURO_STOXX_50\",\n",
    "    \"cac40\": \"https://en.wikipedia.org/wiki/CAC_40\",\n",
    "    \"dax\": \"https://en.wikipedia.org/wiki/DAX\",\n",
    "    \"csi300\": \"https://en.wikipedia.org/wiki/CSI_300_Index\",\n",
    "    \"spla40\": \"https://en.wikipedia.org/wiki/S%26P_Latin_America_40\",\n",
    "    \"bsesensex\": \"https://en.wikipedia.org/wiki/BSE_SENSEX\",\n",
    "    \"nasdaq100\": \"https://en.wikipedia.org/wiki/Nasdaq-100\",\n",
    "}\n",
    "\n",
    "# IMPORTANT: Configure HTTP headers to identify our bot to Wikipedia\n",
    "# This is REQUIRED for ethical web scraping - identify yourself!\n",
    "# Wikipedia may block requests without proper User-Agent headers\n",
    "\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [\n",
    "    (\"User-Agent\", \"MyResearchBot/1.0 (contact@example.com)\")  # Identify your bot\n",
    "]\n",
    "urllib.request.install_opener(opener)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e6a2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing index: sp500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:48.946\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/sp500/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:48.948\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/sp500/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:48.950\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/sp500/table_2.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for sp500 saved.\n",
      "Processing index: eurostoxx50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:49.184\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/eurostoxx50/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.185\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/eurostoxx50/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.186\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/eurostoxx50/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.187\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/eurostoxx50/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.188\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/eurostoxx50/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/eurostoxx50/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.189\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/eurostoxx50/table_6.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.190\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 7 to data/indices/eurostoxx50/table_7.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 8 to data/indices/eurostoxx50/table_8.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.191\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 9 to data/indices/eurostoxx50/table_9.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.351\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/cac40/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.352\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/cac40/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/cac40/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.353\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/cac40/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/cac40/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.354\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/cac40/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.355\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/cac40/table_6.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 7 to data/indices/cac40/table_7.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.356\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 8 to data/indices/cac40/table_8.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 9 to data/indices/cac40/table_9.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.357\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 10 to data/indices/cac40/table_10.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.358\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 11 to data/indices/cac40/table_11.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 12 to data/indices/cac40/table_12.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.359\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 13 to data/indices/cac40/table_13.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.360\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 14 to data/indices/cac40/table_14.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 15 to data/indices/cac40/table_15.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.361\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 16 to data/indices/cac40/table_16.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.362\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 17 to data/indices/cac40/table_17.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.363\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 18 to data/indices/cac40/table_18.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.364\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 19 to data/indices/cac40/table_19.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for eurostoxx50 saved.\n",
      "Processing index: cac40...\n",
      "  -> Done: Components for cac40 saved.\n",
      "Processing index: dax...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:49.521\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/dax/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.522\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/dax/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/dax/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.523\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/dax/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/dax/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/dax/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.525\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/dax/table_6.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 7 to data/indices/dax/table_7.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.527\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 8 to data/indices/dax/table_8.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.528\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 9 to data/indices/dax/table_9.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.700\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/csi300/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/csi300/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.701\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/csi300/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.704\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/csi300/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.705\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/csi300/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/csi300/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.706\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/csi300/table_6.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for dax saved.\n",
      "Processing index: csi300...\n",
      "  -> Done: Components for csi300 saved.\n",
      "Processing index: spla40...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:49.805\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/spla40/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.806\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/spla40/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.807\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/spla40/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:49.809\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/spla40/table_3.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for spla40 saved.\n",
      "Processing index: bsesensex...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:50.023\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/bsesensex/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.025\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/bsesensex/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/bsesensex/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.026\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/bsesensex/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.027\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/bsesensex/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/bsesensex/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.028\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/bsesensex/table_6.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 7 to data/indices/bsesensex/table_7.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.029\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 8 to data/indices/bsesensex/table_8.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.030\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 9 to data/indices/bsesensex/table_9.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 10 to data/indices/bsesensex/table_10.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 11 to data/indices/bsesensex/table_11.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.031\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 12 to data/indices/bsesensex/table_12.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 13 to data/indices/bsesensex/table_13.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.032\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 14 to data/indices/bsesensex/table_14.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 15 to data/indices/bsesensex/table_15.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 16 to data/indices/bsesensex/table_16.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.034\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 17 to data/indices/bsesensex/table_17.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 18 to data/indices/bsesensex/table_18.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.035\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 19 to data/indices/bsesensex/table_19.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.036\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 20 to data/indices/bsesensex/table_20.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 21 to data/indices/bsesensex/table_21.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.037\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 22 to data/indices/bsesensex/table_22.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 23 to data/indices/bsesensex/table_23.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.038\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 24 to data/indices/bsesensex/table_24.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.039\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 25 to data/indices/bsesensex/table_25.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.040\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 26 to data/indices/bsesensex/table_26.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 27 to data/indices/bsesensex/table_27.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.041\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 28 to data/indices/bsesensex/table_28.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 29 to data/indices/bsesensex/table_29.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.042\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 30 to data/indices/bsesensex/table_30.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 31 to data/indices/bsesensex/table_31.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.043\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 32 to data/indices/bsesensex/table_32.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for bsesensex saved.\n",
      "Processing index: nasdaq100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-11-29 14:05:50.401\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 0 to data/indices/nasdaq100/table_0.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.402\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 1 to data/indices/nasdaq100/table_1.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 2 to data/indices/nasdaq100/table_2.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.403\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 3 to data/indices/nasdaq100/table_3.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.404\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 4 to data/indices/nasdaq100/table_4.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 5 to data/indices/nasdaq100/table_5.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.406\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 6 to data/indices/nasdaq100/table_6.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 7 to data/indices/nasdaq100/table_7.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 8 to data/indices/nasdaq100/table_8.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 9 to data/indices/nasdaq100/table_9.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.408\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 10 to data/indices/nasdaq100/table_10.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 11 to data/indices/nasdaq100/table_11.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 12 to data/indices/nasdaq100/table_12.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 13 to data/indices/nasdaq100/table_13.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 14 to data/indices/nasdaq100/table_14.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 15 to data/indices/nasdaq100/table_15.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 16 to data/indices/nasdaq100/table_16.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.411\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 17 to data/indices/nasdaq100/table_17.csv\u001b[0m\n",
      "\u001b[32m2025-11-29 14:05:50.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_index_components\u001b[0m:\u001b[36m45\u001b[0m - \u001b[1mSaved table 18 to data/indices/nasdaq100/table_18.csv\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Done: Components for nasdaq100 saved.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION: Download Index Components\n",
    "# ============================================================================\n",
    "# Loop through each index and extract its company components from Wikipedia\n",
    "# This may take a few minutes depending on internet speed\n",
    "\n",
    "for idx, url in indices.items():\n",
    "    print(f\"Processing index: {idx}...\")\n",
    "    try:\n",
    "        get_index_components(url, f\"./data/indices/{idx}\", opener)\n",
    "        print(f\"  -> Done: Components for {idx} saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"  !! Error processing {idx}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06eb0af",
   "metadata": {},
   "source": [
    "## Step 2: Extract Company Infoboxes from Wikipedia\n",
    "\n",
    "### What are Infoboxes?\n",
    "Wikipedia infoboxes are structured data boxes that appear on the right side of articles. They contain:\n",
    "- Company name and alternative names\n",
    "- Industry classification\n",
    "- Founded date and location\n",
    "- Key executives\n",
    "- Headquarters location\n",
    "- Number of employees\n",
    "- Revenue and financial metrics\n",
    "- Official website URLs\n",
    "- Stock exchange listings\n",
    "- And much more...\n",
    "\n",
    "### Why Infoboxes?\n",
    "- **Structured data**: Unlike article body text, infoboxes are semi-structured\n",
    "- **Consistency**: Fields follow a template across similar articles\n",
    "- **Ease of extraction**: Wikipedia APIs can parse infoboxes directly\n",
    "- **Rich context**: Perfect for LLM prompts - contains exactly the info LLMs need\n",
    "\n",
    "### Process\n",
    "1. Use the `wptools` library to fetch each company's Wikipedia page\n",
    "2. Extract the infobox (structured data) from the page parse\n",
    "3. Save as JSON for flexibility and later processing\n",
    "4. Handle errors gracefully (some companies may not have Wikipedia pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19bc802e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infobox for 3M saved to: ./data/infoboxes/sp500/3M_infobox.json\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXAMPLE: Extract a Single Company Infobox\n",
    "# ============================================================================\n",
    "# This example shows the process for one company (3M) from S&P 500\n",
    "# In production, we'd loop this for all companies\n",
    "\n",
    "\n",
    "# We'll use 3M (company name) as our test case\n",
    "company_name = \"3M\"\n",
    "\n",
    "def extract_save_infobox(company_name, output_folder=\"./data/infoboxes/sp500\"):\n",
    "    \"\"\"\n",
    "    Fetches Wikipedia infobox data for a company and saves as JSON in specified folder.\n",
    "    \"\"\"\n",
    "    wiki_page = wptools.page(company_name, silent=True)\n",
    "    wiki_page.get_parse(show=False)\n",
    "    infobox = wiki_page.data.get('infobox', {})\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    output_path = f\"{output_folder}/{company_name.replace('/', '_')}_infobox.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(infobox, f, ensure_ascii=False, indent=2)\n",
    "    return output_path, infobox\n",
    "\n",
    "# Example usage for 3M\n",
    "output_path, infobox = extract_save_infobox(company_name)\n",
    "\n",
    "print(f\"Infobox for {company_name} saved to: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5adcddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infobox for 3M:\n",
      "{\n",
      "  \"name\": \"3M Company\",\n",
      "  \"logo\": \"3M wordmark.svg\",\n",
      "  \"logo_size\": \"175px\",\n",
      "  \"image\": \"3-M Building Maplewood MN1.jpg\",\n",
      "  \"image_size\": \"250px\",\n",
      "  \"image_caption\": \"3M headquarters in [[Maplewood, Minnesota]]\",\n",
      "  \"former_name\": \"Minnesota Mining and Manufacturing Company (1902\\u20132002)\",\n",
      "  \"type\": \"[[Public company|Public]]\",\n",
      "  \"traded_as\": \"{{Unbulleted list|New York Stock Exchange|MMM|[[Dow Jones Industrial Average|DJIA]] component|[[S&P 100]] component|[[S&P 500]] component}} {{New York Stock Exchange|MMM}}\",\n",
      "  \"ISIN\": \"{{ISIN|sl|=|n|pl|=|y|US88579Y1010}}\",\n",
      "  \"industry\": \"[[Conglomerate (company)|Conglomerate]]\",\n",
      "  \"foundation\": \"{{Start date and age|1902|6|13}} in [[Two Harbors, Minnesota]], U.S.\",\n",
      "  \"founders\": \"{{Unbulleted list|J. Danley Budd|Henry S. Bryan|William A. McGonagle|John Dwan|Hermon W. Cable | Charles Simmons|ref|{{cite web |url=https://www.3m.com.au/3M/en_AU/company-au/news-releases/full-story/?storyid=51f5cfac-3ea9-4a98-a406-e2b955c3fd40 |title=It all started with a rock |date=June 11, 2021 |work=3M Australia |access-date=March 9, 2022}}|</ref>}}\",\n",
      "  \"location_city\": \"[[Maplewood, Minnesota]]\",\n",
      "  \"location_country\": \"U.S.\",\n",
      "  \"area_served\": \"Worldwide\",\n",
      "  \"key_people\": \"{{plainlist|\\n* [[Michael F. Roman]] (chairman)\\n* [[William M. Brown (businessman)|William M. Brown]] (CEO)|ref|{{citation|url=https://www.manufacturingdive.com/news/3m-ceo-william-brown-executive-chair-michael-roman/709987/|website=Manufacturing Dive|date=March 12, 2024 |title=3M appoints new CEO }}|</ref>}}\",\n",
      "  \"revenue\": \"{{decrease}} {{US$|24.58 billion|link|=|yes}} (2024)\",\n",
      "  \"operating_income\": \"{{increase}} {{US$|4.822 billion}} (2024)\",\n",
      "  \"net_income\": \"{{increase}} {{US$|4.173 billion}} (2024)\",\n",
      "  \"assets\": \"{{decrease}} {{US$|39.87 billion}} (2024)\",\n",
      "  \"equity\": \"{{decrease}} {{US$|3.842 billion}} (2024)\",\n",
      "  \"num_employees\": \"{{circa|61,500}} (2024)\",\n",
      "  \"website\": \"{{URL|3m.com}}\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# DISPLAY: View the Extracted Infobox\n",
    "# ============================================================================\n",
    "# This shows what data we extracted from Wikipedia\n",
    "\n",
    "# Display the infobox data previously saved for 3M (or any company) as an example\n",
    "\n",
    "print(f\"Infobox for {company_name}:\")\n",
    "print(json.dumps(infobox, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33f8f7d",
   "metadata": {},
   "source": [
    "## Step 3: Aggregate Infoboxes into Databases\n",
    "\n",
    "### What We're Building\n",
    "We're converting individual JSON files (one per company) into consolidated CSV databases (one per index).\n",
    "\n",
    "### Why?\n",
    "- **Easier analysis**: CSV format works with pandas, Excel, and most analysis tools\n",
    "- **Efficiency**: One file per index instead of hundreds of individual JSON files\n",
    "- **Standardization**: Creates a uniform dataset structure for LLM processing\n",
    "\n",
    "### Process\n",
    "1. Read all JSON infobox files for an index from disk\n",
    "2. Convert each JSON to a DataFrame row\n",
    "3. Concatenate all rows into a single DataFrame\n",
    "4. Save as CSV with proper encoding\n",
    "\n",
    "### Notes for Future Enhancement\n",
    "- The infoboxes contain many fields beyond what we use now (URLs, images, etc.)\n",
    "- Future work could extract and leverage additional information\n",
    "- This foundation allows flexible data extraction later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3394df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing sp500: 100%|██████████| 1/1 [00:00<00:00, 652.30it/s]\n",
      "\u001b[32m2025-11-29 14:05:53.579\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36maggregate_infoboxes\u001b[0m:\u001b[36m68\u001b[0m - \u001b[1msp500: saved 1 rows to data/databases/sp500_infoboxes.csv. Skipped 0 files.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> sp500: saved 1 rows to data/databases/sp500_infoboxes.csv. Skipped 0 files.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION: Merge All Infoboxes into Index Databases\n",
    "# ============================================================================\n",
    "# Loop through each index folder and consolidate all JSON infoboxes into CSV\n",
    "\n",
    "def aggregate_infoboxes(index_name: str, infobox_dir: str, output_dir: str = \"./data/databases\") -> None:\n",
    "    \"\"\"\n",
    "    Aggregate all JSON infobox files for an index into a single CSV database.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    index_name : str\n",
    "        Name of the index (e.g., 'sp500')\n",
    "    infobox_dir : str\n",
    "        Directory containing JSON infobox files\n",
    "    output_dir : str\n",
    "        Directory where consolidated CSV will be saved\n",
    "    \"\"\"\n",
    "    infobox_path = Path(infobox_dir)\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    if not infobox_path.exists():\n",
    "        logger.warning(f\"Infobox directory {infobox_dir} does not exist. Skipping {index_name}.\")\n",
    "        return\n",
    "    \n",
    "    # Find all JSON files in the infobox directory\n",
    "    json_files = list(infobox_path.glob(\"*_infobox.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        logger.warning(f\"No infobox JSON files found in {infobox_dir}\")\n",
    "        return\n",
    "    \n",
    "    rows = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for json_file in tqdm(json_files, desc=f\"Processing {index_name}\"):\n",
    "        try:\n",
    "            with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                infobox_data = json.load(f)\n",
    "            \n",
    "            # Add the company name (from filename) and index name\n",
    "            company_name = json_file.stem.replace('_infobox', '')\n",
    "            row = {'company_name': company_name, 'index': index_name}\n",
    "            \n",
    "            # Flatten the infobox dictionary into the row\n",
    "            for key, value in infobox_data.items():\n",
    "                # Convert value to string if it's not already\n",
    "                if isinstance(value, (dict, list)):\n",
    "                    row[key] = json.dumps(value, ensure_ascii=False)\n",
    "                else:\n",
    "                    row[key] = str(value) if value is not None else ''\n",
    "            \n",
    "            rows.append(row)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {json_file}: {e}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "    \n",
    "    if rows:\n",
    "        # Create DataFrame from all rows\n",
    "        df = pd.DataFrame(rows)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = output_path / f\"{index_name}_infoboxes.csv\"\n",
    "        df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "        logger.info(f\"{index_name}: saved {len(rows)} rows to {csv_path}. Skipped {skipped} files.\")\n",
    "        print(f\">>> {index_name}: saved {len(rows)} rows to {csv_path}. Skipped {skipped} files.\")\n",
    "    else:\n",
    "        logger.warning(f\"No valid infoboxes found for {index_name}\")\n",
    "\n",
    "# Process S&P 500 infoboxes (example - you can loop through all indices)\n",
    "aggregate_infoboxes(\"sp500\", \"./data/infoboxes/sp500\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21235e6",
   "metadata": {},
   "source": [
    "# Part 2: Data Processing & LLM Prompt Engineering\n",
    "\n",
    "## Overview of Part 2\n",
    "Now that we have structured company data from Wikipedia, we'll:\n",
    "\n",
    "1. **Load and analyze** the infobox database\n",
    "2. **Clean and preprocess** the data for LLM consumption\n",
    "3. **Create prompt templates** for different LLM tasks\n",
    "4. **Design context formatting** that maximizes LLM effectiveness\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Why Clean Data for LLMs?\n",
    "- LLMs perform better with well-structured, clean text\n",
    "- Removing noise and formatting artifacts improves accuracy\n",
    "- Consistent formatting allows for better prompt engineering\n",
    "- Clean data enables batch processing and cost optimization\n",
    "\n",
    "### Prompt Engineering\n",
    "Prompt engineering is the art of crafting inputs to LLMs to get better outputs. We'll explore:\n",
    "- **Context formatting**: How to present company data effectively\n",
    "- **Task-specific templates**: Different prompts for different goals\n",
    "- **Few-shot learning**: Providing examples to guide LLM behavior\n",
    "- **Output structuring**: Getting structured responses (JSON, tables, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7152b26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded infobox dataset with 1 rows and 26 columns.\n",
      "\n",
      "================================================================================\n",
      "DATASET OVERVIEW\n",
      "================================================================================\n",
      "\n",
      "Columns (26):\n",
      "   1. company_name                   (1 non-null values)\n",
      "   2. index                          (1 non-null values)\n",
      "   3. name                           (1 non-null values)\n",
      "   4. logo                           (1 non-null values)\n",
      "   5. logo_size                      (1 non-null values)\n",
      "   6. image                          (1 non-null values)\n",
      "   7. image_size                     (1 non-null values)\n",
      "   8. image_caption                  (1 non-null values)\n",
      "   9. former_name                    (1 non-null values)\n",
      "  10. type                           (1 non-null values)\n",
      "  11. traded_as                      (1 non-null values)\n",
      "  12. ISIN                           (1 non-null values)\n",
      "  13. industry                       (1 non-null values)\n",
      "  14. foundation                     (1 non-null values)\n",
      "  15. founders                       (1 non-null values)\n",
      "  16. location_city                  (1 non-null values)\n",
      "  17. location_country               (1 non-null values)\n",
      "  18. area_served                    (1 non-null values)\n",
      "  19. key_people                     (1 non-null values)\n",
      "  20. revenue                        (1 non-null values)\n",
      "  21. operating_income               (1 non-null values)\n",
      "  22. net_income                     (1 non-null values)\n",
      "  23. assets                         (1 non-null values)\n",
      "  24. equity                         (1 non-null values)\n",
      "  25. num_employees                  (1 non-null values)\n",
      "  26. website                        (1 non-null values)\n",
      "\n",
      "\n",
      "First few rows:\n",
      "  company_name  index        name             logo logo_size  \\\n",
      "0           3M  sp500  3M Company  3M wordmark.svg     175px   \n",
      "\n",
      "                            image image_size  \\\n",
      "0  3-M Building Maplewood MN1.jpg      250px   \n",
      "\n",
      "                                 image_caption  \\\n",
      "0  3M headquarters in [[Maplewood, Minnesota]]   \n",
      "\n",
      "                                         former_name  \\\n",
      "0  Minnesota Mining and Manufacturing Company (19...   \n",
      "\n",
      "                        type  ... location_country area_served  \\\n",
      "0  [[Public company|Public]]  ...             U.S.   Worldwide   \n",
      "\n",
      "                                          key_people  \\\n",
      "0  {{plainlist|\\n* [[Michael F. Roman]] (chairman...   \n",
      "\n",
      "                                             revenue  \\\n",
      "0  {{decrease}} {{US$|24.58 billion|link|=|yes}} ...   \n",
      "\n",
      "                            operating_income  \\\n",
      "0  {{increase}} {{US$|4.822 billion}} (2024)   \n",
      "\n",
      "                                  net_income  \\\n",
      "0  {{increase}} {{US$|4.173 billion}} (2024)   \n",
      "\n",
      "                                      assets  \\\n",
      "0  {{decrease}} {{US$|39.87 billion}} (2024)   \n",
      "\n",
      "                                      equity            num_employees  \\\n",
      "0  {{decrease}} {{US$|3.842 billion}} (2024)  {{circa|61,500}} (2024)   \n",
      "\n",
      "          website  \n",
      "0  {{URL|3m.com}}  \n",
      "\n",
      "[1 rows x 26 columns]\n",
      "\n",
      "\n",
      "Data types:\n",
      "company_name        object\n",
      "index               object\n",
      "name                object\n",
      "logo                object\n",
      "logo_size           object\n",
      "image               object\n",
      "image_size          object\n",
      "image_caption       object\n",
      "former_name         object\n",
      "type                object\n",
      "traded_as           object\n",
      "ISIN                object\n",
      "industry            object\n",
      "foundation          object\n",
      "founders            object\n",
      "location_city       object\n",
      "location_country    object\n",
      "area_served         object\n",
      "key_people          object\n",
      "revenue             object\n",
      "operating_income    object\n",
      "net_income          object\n",
      "assets              object\n",
      "equity              object\n",
      "num_employees       object\n",
      "website             object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Missing values per column:\n",
      "Empty DataFrame\n",
      "Columns: [Missing Count, Missing %]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 4: LOAD AND ANALYZE INFOBOX DATA\n",
    "# ============================================================================\n",
    "# Now we'll load the consolidated CSV database and analyze its structure\n",
    "\n",
    "def load_infobox_database(csv_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load infobox database from CSV file.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    csv_path : str\n",
    "        Path to the CSV file containing infobox data\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing the infobox data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path, encoding='utf-8')\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"File not found: {csv_path}\")\n",
    "        # Try alternative path\n",
    "        alt_path = f\"./data/databases/{Path(csv_path).name}\"\n",
    "        if Path(alt_path).exists():\n",
    "            logger.info(f\"Trying alternative path: {alt_path}\")\n",
    "            return pd.read_csv(alt_path, encoding='utf-8')\n",
    "        raise\n",
    "\n",
    "def analyze_infobox_data(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Display basic statistics and information about the infobox dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        DataFrame containing infobox data\n",
    "    \"\"\"\n",
    "    print(f\"Loaded infobox dataset with {df.shape[0]:,} rows and {df.shape[1]:,} columns.\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nColumns ({len(df.columns)}):\")\n",
    "    for i, col in enumerate(df.columns, 1):\n",
    "        non_null = df[col].notna().sum()\n",
    "        print(f\"  {i:2d}. {col:30s} ({non_null:,} non-null values)\")\n",
    "    \n",
    "    print(f\"\\n\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(f\"\\n\\nData types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(f\"\\n\\nMissing values per column:\")\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Missing %': missing_pct\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False))\n",
    "\n",
    "# Load the S&P 500 infobox data (CSV format)\n",
    "csv_path = \"./data/databases/sp500_infoboxes.csv\"\n",
    "df = load_infobox_database(csv_path)\n",
    "\n",
    "# Display basic statistics about the dataset\n",
    "analyze_infobox_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd6d22b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE COMPANY DATA PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "1. Original Row (first few fields):\n",
      "company_name                                                    3M\n",
      "index                                                        sp500\n",
      "name                                                    3M Company\n",
      "logo                                               3M wordmark.svg\n",
      "logo_size                                                    175px\n",
      "image                               3-M Building Maplewood MN1.jpg\n",
      "image_size                                                   250px\n",
      "image_caption          3M headquarters in [[Maplewood, Minnesota]]\n",
      "former_name      Minnesota Mining and Manufacturing Company (19...\n",
      "type                                     [[Public company|Public]]\n",
      "Name: 0, dtype: object\n",
      "\n",
      "2. Extracted Key Facts:\n",
      "   name                : 3M Company\n",
      "   type                : Public\n",
      "   industry            : Conglomerate\n",
      "   founded             : in Two Harbors, Minnesota, U.S.\n",
      "   founders            :  \n",
      "   headquarters        : Maplewood, Minnesota\n",
      "   key_people          :  \n",
      "   employees           : (2024)\n",
      "   revenue             : (2024)\n",
      "   website             : \n",
      "   stock_exchange      : \n",
      "\n",
      "3. Formatted Context for LLM:\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 5: DATA PREPROCESSING FOR LLM\n",
    "# ============================================================================\n",
    "# Functions to clean and format data for Large Language Models\n",
    "\n",
    "from typing import Dict\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Clean and normalize text for LLM input.\n",
    "    \n",
    "    Removes Wikipedia markup, extra whitespace, and formatting artifacts.\n",
    "    This ensures clean, readable text for LLMs.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Raw text from Wikipedia infobox\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Cleaned text suitable for LLM processing\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text is None:\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove Wikipedia template syntax {{...}}\n",
    "    text = re.sub(r'\\{\\{[^}]*\\}\\}', '', text)\n",
    "    \n",
    "    # Remove Wikipedia link syntax [[...|...]] or [[...]]\n",
    "    text = re.sub(r'\\[\\[([^\\]]*\\|)?([^\\]]+)\\]\\]', r'\\2', text)\n",
    "    \n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Remove extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    # Remove common Wikipedia artifacts\n",
    "    text = text.replace('{{', '').replace('}}', '')\n",
    "    text = text.replace('|', ' ')\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_key_facts(row: pd.Series) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Extract key facts from a company infobox row.\n",
    "    \n",
    "    Focuses on most important fields for LLM processing:\n",
    "    - name, type, industry (what the company is)\n",
    "    - founded, founder (history)\n",
    "    - headquarters (location)\n",
    "    - key_people (leadership)\n",
    "    - employees, revenue (size/scale)\n",
    "    - website, stock_exchange (external links)\n",
    "    \n",
    "    Uses case-insensitive matching to handle Wikipedia's varying field names.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : pd.Series\n",
    "        A row from the infobox DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dict[str, str]\n",
    "        Dictionary of key facts with cleaned values\n",
    "    \"\"\"\n",
    "    facts = {}\n",
    "    \n",
    "    # Define key fields to extract (with variations)\n",
    "    key_fields = {\n",
    "        'name': ['name', 'company_name', 'title'],\n",
    "        'type': ['type'],\n",
    "        'industry': ['industry', 'sector', 'business'],\n",
    "        'founded': ['founded', 'foundation', 'established', 'founded_date'],\n",
    "        'founders': ['founders', 'founder'],\n",
    "        'headquarters': ['headquarters', 'location_city', 'location_country', 'hq'],\n",
    "        'key_people': ['key_people', 'ceo', 'leadership', 'executives'],\n",
    "        'employees': ['num_employees', 'employees', 'employee_count'],\n",
    "        'revenue': ['revenue', 'revenue_usd', 'annual_revenue'],\n",
    "        'website': ['website', 'url', 'homepage'],\n",
    "        'stock_exchange': ['traded_as', 'stock_exchange', 'exchange', 'ticker']\n",
    "    }\n",
    "    \n",
    "    # Extract values using case-insensitive matching\n",
    "    row_lower = {str(k).lower(): v for k, v in row.items()}\n",
    "    \n",
    "    for fact_name, field_variations in key_fields.items():\n",
    "        for field_var in field_variations:\n",
    "            # Try exact match first\n",
    "            if field_var in row:\n",
    "                value = row[field_var]\n",
    "                if pd.notna(value) and str(value).strip():\n",
    "                    facts[fact_name] = clean_text(str(value))\n",
    "                    break\n",
    "            # Try case-insensitive match\n",
    "            elif field_var.lower() in row_lower:\n",
    "                for orig_key, value in row.items():\n",
    "                    if str(orig_key).lower() == field_var.lower():\n",
    "                        if pd.notna(value) and str(value).strip():\n",
    "                            facts[fact_name] = clean_text(str(value))\n",
    "                            break\n",
    "                if fact_name in facts:\n",
    "                    break\n",
    "    \n",
    "    return facts\n",
    "\n",
    "\n",
    "def row_to_context(row: pd.Series) -> str:\n",
    "    \"\"\"\n",
    "    Convert a company row into a formatted context string for LLM input.\n",
    "    \n",
    "    Takes a DataFrame row and produces nicely formatted text that\n",
    "    an LLM can understand and analyze. Format makes it clear what\n",
    "    information is being provided.\n",
    "    \n",
    "    Output format:\n",
    "    COMPANY INFORMATION:\n",
    "    --\n",
    "    FIELD: value\n",
    "    FIELD: value\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    row : pd.Series\n",
    "        A row from the infobox DataFrame\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Formatted context string for LLM\n",
    "    \"\"\"\n",
    "    facts = extract_key_facts(row)\n",
    "    \n",
    "    if not facts:\n",
    "        return \"COMPANY INFORMATION:\\n--\\nNo information available.\"\n",
    "    \n",
    "    # Get company name (prefer 'name', fallback to 'company_name')\n",
    "    company_name = facts.get('name', row.get('company_name', 'Unknown Company'))\n",
    "    \n",
    "    context_lines = [f\"COMPANY INFORMATION: {company_name}\", \"--\"]\n",
    "    \n",
    "    # Add each fact as a formatted line\n",
    "    for key, value in facts.items():\n",
    "        if value and value.strip():\n",
    "            # Format field name nicely (capitalize, replace underscores)\n",
    "            field_name = key.replace('_', ' ').title()\n",
    "            context_lines.append(f\"{field_name}: {value}\")\n",
    "    \n",
    "    return \"\\n\".join(context_lines)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# TEST: Demonstrate preprocessing on sample company\n",
    "# ============================================================================\n",
    "\n",
    "# Test on the first row of the dataset (or a specific company)\n",
    "if not df.empty:\n",
    "    sample_row = df.iloc[0]\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"SAMPLE COMPANY DATA PREPROCESSING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. Original Row (first few fields):\")\n",
    "    print(sample_row.head(10))\n",
    "    \n",
    "    print(\"\\n2. Extracted Key Facts:\")\n",
    "    key_facts = extract_key_facts(sample_row)\n",
    "    for key, value in key_facts.items():\n",
    "        print(f\"   {key:20s}: {value[:80]}...\" if len(value) > 80 else f\"   {key:20s}: {value}\")\n",
    "    \n",
    "    print(\"\\n3. Formatted Context for LLM:\")\n",
    "    context = row_to_context(sample_row)\n",
    "    print(context)\n",
    "else:\n",
    "    print(\"No data available for testing. Please ensure infoboxes have been extracted.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b5f1683b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT TEMPLATE EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "1. QA Prompt:\n",
      "You are a financial analyst assistant. Answer the following question based on the company information provided.\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Question: What industry does this company operate in?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "2. Classification Prompt:\n",
      "You are a business classification expert. Classify the following company into one of these categories: Large Cap, Mid Cap, Small Cap, Unknown\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Classification (choose one from: Large Cap, Mid Cap, Small Cap, Unknown):\n",
      "\n",
      "\n",
      "3. Summarization Prompt:\n",
      "You are a business analyst. Summarize the following company information. Focus on financial metrics, revenue, profitability, and financial health.\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Summary:\n",
      "\n",
      "\n",
      "4. Extraction Prompt:\n",
      "You are a data extraction specialist. Extract the following information from the company data and return it as JSON format.\n",
      "\n",
      "Fields to extract: revenue, employees, founded_year\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Extracted information (JSON format):\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# STEP 6: CREATE PROMPT TEMPLATES FOR LLM TASKS\n",
    "# ============================================================================\n",
    "# Different tasks require different prompt structures. Build task-specific\n",
    "# prompt templates that can be reused across your dataset.\n",
    "\n",
    "def create_qa_prompt(company_context: str, question: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a question-answering prompt for LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    company_context : str\n",
    "        Formatted company information context\n",
    "    question : str\n",
    "        Question to ask about the company\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt for QA task\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a financial analyst assistant. Answer the following question based on the company information provided.\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_classification_prompt(company_context: str, categories: list) -> str:\n",
    "    \"\"\"\n",
    "    Create a classification prompt for LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    company_context : str\n",
    "        Formatted company information context\n",
    "    categories : list\n",
    "        List of categories to classify into\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt for classification task\n",
    "    \"\"\"\n",
    "    categories_str = \", \".join(categories)\n",
    "    prompt = f\"\"\"You are a business classification expert. Classify the following company into one of these categories: {categories_str}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Classification (choose one from: {categories_str}):\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_summarization_prompt(company_context: str, focus: str = \"general\") -> str:\n",
    "    \"\"\"\n",
    "    Create a summarization prompt for LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    company_context : str\n",
    "        Formatted company information context\n",
    "    focus : str\n",
    "        Focus area for summary (e.g., \"financial\", \"operations\", \"general\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt for summarization task\n",
    "    \"\"\"\n",
    "    focus_instructions = {\n",
    "        \"financial\": \"Focus on financial metrics, revenue, profitability, and financial health.\",\n",
    "        \"operations\": \"Focus on business operations, industry, products, and market position.\",\n",
    "        \"general\": \"Provide a comprehensive overview of the company.\"\n",
    "    }\n",
    "    \n",
    "    instruction = focus_instructions.get(focus, focus_instructions[\"general\"])\n",
    "    \n",
    "    prompt = f\"\"\"You are a business analyst. Summarize the following company information. {instruction}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Summary:\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_comparison_prompt(company1_context: str, company2_context: str, aspect: str = \"general\") -> str:\n",
    "    \"\"\"\n",
    "    Create a comparison prompt for LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    company1_context : str\n",
    "        Formatted information for first company\n",
    "    company2_context : str\n",
    "        Formatted information for second company\n",
    "    aspect : str\n",
    "        Aspect to compare (e.g., \"financial\", \"size\", \"industry\")\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt for comparison task\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"You are a comparative business analyst. Compare the following two companies, focusing on {aspect} aspects.\n",
    "\n",
    "COMPANY 1:\n",
    "{company1_context}\n",
    "\n",
    "COMPANY 2:\n",
    "{company2_context}\n",
    "\n",
    "Comparison (focusing on {aspect}):\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def create_extraction_prompt(company_context: str, fields_to_extract: list) -> str:\n",
    "    \"\"\"\n",
    "    Create an information extraction prompt for LLM.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    company_context : str\n",
    "        Formatted company information context\n",
    "    fields_to_extract : list\n",
    "        List of specific fields to extract\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Complete prompt for extraction task\n",
    "    \"\"\"\n",
    "    fields_str = \", \".join(fields_to_extract)\n",
    "    prompt = f\"\"\"You are a data extraction specialist. Extract the following information from the company data and return it as JSON format.\n",
    "\n",
    "Fields to extract: {fields_str}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Extracted information (JSON format):\"\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if not df.empty:\n",
    "    sample_context = row_to_context(df.iloc[0])\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PROMPT TEMPLATE EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. QA Prompt:\")\n",
    "    print(create_qa_prompt(sample_context, \"What industry does this company operate in?\"))\n",
    "    \n",
    "    print(\"\\n\\n2. Classification Prompt:\")\n",
    "    print(create_classification_prompt(sample_context, [\"Large Cap\", \"Mid Cap\", \"Small Cap\", \"Unknown\"]))\n",
    "    \n",
    "    print(\"\\n\\n3. Summarization Prompt:\")\n",
    "    print(create_summarization_prompt(sample_context, \"financial\"))\n",
    "    \n",
    "    print(\"\\n\\n4. Extraction Prompt:\")\n",
    "    print(create_extraction_prompt(sample_context, [\"revenue\", \"employees\", \"founded_year\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca896f4",
   "metadata": {},
   "source": [
    "Create a PromptBuilder class with different methods for creating prompts:\n",
    "- qa prompt\n",
    "- classification prompt\n",
    "- summarization prompt\n",
    "- comparison prompt\n",
    "- information extraction prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd8ec58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROMPT BUILDER CLASS EXAMPLES\n",
      "================================================================================\n",
      "\n",
      "1. QA Prompt:\n",
      "You are an expert financial analyst with deep knowledge of corporate structures and markets.\n",
      "\n",
      "Based on the following company information, answer the question.\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Question: What is the company's primary industry?\n",
      "\n",
      "Answer:\n",
      "\n",
      "\n",
      "2. Classification Prompt:\n",
      "You are an expert financial analyst with deep knowledge of corporate structures and markets.\n",
      "\n",
      "Classify the company's primary sector the following company into one of these categories: \"Technology\", \"Finance\", \"Manufacturing\", \"Retail\", \"Other\"\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Classification (choose one from: \"Technology\", \"Finance\", \"Manufacturing\", \"Retail\", \"Other\"):\n",
      "\n",
      "\n",
      "3. Summarization Prompt (Financial Focus):\n",
      "You are an expert financial analyst with deep knowledge of corporate structures and markets.\n",
      "\n",
      "Summarize the following company information. Focus on financial metrics, revenue, profitability, and financial health. Provide a detailed summary (1-2 paragraphs).\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Summary:\n",
      "\n",
      "\n",
      "4. Information Extraction Prompt:\n",
      "You are an expert financial analyst with deep knowledge of corporate structures and markets.\n",
      "\n",
      "Extract the following information from the company data: revenue, employees, founded_year, headquarters\n",
      "\n",
      "Return the extracted information as a JSON object.\n",
      "\n",
      "COMPANY INFORMATION: 3M Company\n",
      "--\n",
      "Name: 3M Company\n",
      "Type: Public\n",
      "Industry: Conglomerate\n",
      "Founded: in Two Harbors, Minnesota, U.S.\n",
      "Headquarters: Maplewood, Minnesota\n",
      "Employees: (2024)\n",
      "Revenue: (2024)\n",
      "\n",
      "Extracted Information:\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PROMPT BUILDER CLASS\n",
    "# ============================================================================\n",
    "# A class-based approach for building prompts with consistent structure\n",
    "\n",
    "class PromptBuilder:\n",
    "    \"\"\"\n",
    "    A class for building structured prompts for LLM tasks.\n",
    "    Provides methods for different types of prompts with consistent formatting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, system_role: str = \"You are a helpful financial analyst assistant.\"):\n",
    "        \"\"\"\n",
    "        Initialize the PromptBuilder.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        system_role : str\n",
    "            System role/identity for the LLM\n",
    "        \"\"\"\n",
    "        self.system_role = system_role\n",
    "    \n",
    "    def qa_prompt(self, company_context: str, question: str, include_examples: bool = False) -> str:\n",
    "        \"\"\"\n",
    "        Create a question-answering prompt.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company_context : str\n",
    "            Formatted company information\n",
    "        question : str\n",
    "            Question to answer\n",
    "        include_examples : bool\n",
    "            Whether to include few-shot examples\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete QA prompt\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"{self.system_role}\n",
    "\n",
    "Based on the following company information, answer the question.\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        if include_examples:\n",
    "            prompt = self._add_qa_examples(prompt)\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def classification_prompt(self, company_context: str, categories: list, \n",
    "                            task_description: str = \"classify\") -> str:\n",
    "        \"\"\"\n",
    "        Create a classification prompt.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company_context : str\n",
    "            Formatted company information\n",
    "        categories : list\n",
    "            List of categories to choose from\n",
    "        task_description : str\n",
    "            Description of the classification task\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete classification prompt\n",
    "        \"\"\"\n",
    "        categories_str = \", \".join([f'\"{cat}\"' for cat in categories])\n",
    "        prompt = f\"\"\"{self.system_role}\n",
    "\n",
    "{task_description.capitalize()} the following company into one of these categories: {categories_str}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Classification (choose one from: {categories_str}):\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def summarization_prompt(self, company_context: str, focus: str = \"general\", \n",
    "                           length: str = \"brief\") -> str:\n",
    "        \"\"\"\n",
    "        Create a summarization prompt.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company_context : str\n",
    "            Formatted company information\n",
    "        focus : str\n",
    "            Focus area (financial, operations, general)\n",
    "        length : str\n",
    "            Desired length (brief, detailed, comprehensive)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete summarization prompt\n",
    "        \"\"\"\n",
    "        focus_map = {\n",
    "            \"financial\": \"Focus on financial metrics, revenue, profitability, and financial health.\",\n",
    "            \"operations\": \"Focus on business operations, industry, products, and market position.\",\n",
    "            \"general\": \"Provide a comprehensive overview.\"\n",
    "        }\n",
    "        \n",
    "        length_map = {\n",
    "            \"brief\": \"Provide a brief summary (2-3 sentences).\",\n",
    "            \"detailed\": \"Provide a detailed summary (1-2 paragraphs).\",\n",
    "            \"comprehensive\": \"Provide a comprehensive summary covering all key aspects.\"\n",
    "        }\n",
    "        \n",
    "        focus_instruction = focus_map.get(focus, focus_map[\"general\"])\n",
    "        length_instruction = length_map.get(length, length_map[\"brief\"])\n",
    "        \n",
    "        prompt = f\"\"\"{self.system_role}\n",
    "\n",
    "Summarize the following company information. {focus_instruction} {length_instruction}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Summary:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def comparison_prompt(self, company1_context: str, company2_context: str, \n",
    "                         aspect: str = \"general\", output_format: str = \"text\") -> str:\n",
    "        \"\"\"\n",
    "        Create a comparison prompt.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company1_context : str\n",
    "            First company's information\n",
    "        company2_context : str\n",
    "            Second company's information\n",
    "        aspect : str\n",
    "            Aspect to compare (financial, size, industry, etc.)\n",
    "        output_format : str\n",
    "            Desired output format (text, json, table)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete comparison prompt\n",
    "        \"\"\"\n",
    "        format_instruction = {\n",
    "            \"text\": \"Provide a written comparison.\",\n",
    "            \"json\": \"Provide the comparison in JSON format with fields: similarities, differences, conclusion.\",\n",
    "            \"table\": \"Provide the comparison in a table format.\"\n",
    "        }.get(output_format, \"Provide a written comparison.\")\n",
    "        \n",
    "        prompt = f\"\"\"{self.system_role}\n",
    "\n",
    "Compare the following two companies, focusing on {aspect} aspects. {format_instruction}\n",
    "\n",
    "COMPANY 1:\n",
    "{company1_context}\n",
    "\n",
    "COMPANY 2:\n",
    "{company2_context}\n",
    "\n",
    "Comparison:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def information_extraction_prompt(self, company_context: str, fields_to_extract: list, \n",
    "                                    output_format: str = \"json\") -> str:\n",
    "        \"\"\"\n",
    "        Create an information extraction prompt.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company_context : str\n",
    "            Formatted company information\n",
    "        fields_to_extract : list\n",
    "            List of fields to extract\n",
    "        output_format : str\n",
    "            Desired output format (json, csv, table)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Complete extraction prompt\n",
    "        \"\"\"\n",
    "        fields_str = \", \".join(fields_to_extract)\n",
    "        \n",
    "        format_instruction = {\n",
    "            \"json\": \"Return the extracted information as a JSON object.\",\n",
    "            \"csv\": \"Return the extracted information as CSV format.\",\n",
    "            \"table\": \"Return the extracted information as a table.\"\n",
    "        }.get(output_format, \"Return the extracted information as a JSON object.\")\n",
    "        \n",
    "        prompt = f\"\"\"{self.system_role}\n",
    "\n",
    "Extract the following information from the company data: {fields_str}\n",
    "\n",
    "{format_instruction}\n",
    "\n",
    "{company_context}\n",
    "\n",
    "Extracted Information:\"\"\"\n",
    "        return prompt\n",
    "    \n",
    "    def _add_qa_examples(self, prompt: str) -> str:\n",
    "        \"\"\"Add few-shot examples to QA prompt.\"\"\"\n",
    "        examples = \"\"\"\n",
    "\n",
    "Examples:\n",
    "Q: What is the company's revenue?\n",
    "A: The company's revenue is $24.58 billion (2024).\n",
    "\n",
    "Q: When was the company founded?\n",
    "A: The company was founded in 1902.\"\"\"\n",
    "        return prompt.replace(\"Answer:\", \"Answer:\" + examples)\n",
    "    \n",
    "    def batch_prompt(self, company_contexts: list, task: str, **kwargs) -> list:\n",
    "        \"\"\"\n",
    "        Create multiple prompts for batch processing.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        company_contexts : list\n",
    "            List of formatted company contexts\n",
    "        task : str\n",
    "            Task type (qa, classification, summarization, etc.)\n",
    "        **kwargs\n",
    "            Additional arguments for the specific task method\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list\n",
    "            List of prompts\n",
    "        \"\"\"\n",
    "        prompts = []\n",
    "        method_map = {\n",
    "            \"qa\": self.qa_prompt,\n",
    "            \"classification\": self.classification_prompt,\n",
    "            \"summarization\": self.summarization_prompt,\n",
    "            \"extraction\": self.information_extraction_prompt\n",
    "        }\n",
    "        \n",
    "        method = method_map.get(task.lower())\n",
    "        if not method:\n",
    "            raise ValueError(f\"Unknown task: {task}. Available: {list(method_map.keys())}\")\n",
    "        \n",
    "        for context in company_contexts:\n",
    "            prompt = method(context, **kwargs)\n",
    "            prompts.append(prompt)\n",
    "        \n",
    "        return prompts\n",
    "\n",
    "\n",
    "# Example usage of PromptBuilder\n",
    "if not df.empty:\n",
    "    builder = PromptBuilder(system_role=\"You are an expert financial analyst with deep knowledge of corporate structures and markets.\")\n",
    "    \n",
    "    sample_context = row_to_context(df.iloc[0])\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"PROMPT BUILDER CLASS EXAMPLES\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(\"\\n1. QA Prompt:\")\n",
    "    print(builder.qa_prompt(sample_context, \"What is the company's primary industry?\"))\n",
    "    \n",
    "    print(\"\\n\\n2. Classification Prompt:\")\n",
    "    print(builder.classification_prompt(\n",
    "        sample_context, \n",
    "        [\"Technology\", \"Finance\", \"Manufacturing\", \"Retail\", \"Other\"],\n",
    "        task_description=\"Classify the company's primary sector\"\n",
    "    ))\n",
    "    \n",
    "    print(\"\\n\\n3. Summarization Prompt (Financial Focus):\")\n",
    "    print(builder.summarization_prompt(sample_context, focus=\"financial\", length=\"detailed\"))\n",
    "    \n",
    "    print(\"\\n\\n4. Information Extraction Prompt:\")\n",
    "    print(builder.information_extraction_prompt(\n",
    "        sample_context,\n",
    "        [\"revenue\", \"employees\", \"founded_year\", \"headquarters\"],\n",
    "        output_format=\"json\"\n",
    "    ))\n",
    "    \n",
    "    if len(df) >= 2:\n",
    "        print(\"\\n\\n5. Comparison Prompt:\")\n",
    "        context2 = row_to_context(df.iloc[1])\n",
    "        print(builder.comparison_prompt(sample_context, context2, aspect=\"financial\", output_format=\"text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f89b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 7: SUMMARY & BEST PRACTICES GUIDE\n",
    "# ============================================================================\n",
    "# Comprehensive guide to using Wikipedia data with LLMs\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "╔════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    LLM PROMPT ENGINEERING SUMMARY                         ║\n",
    "╚════════════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "\n",
    "🎯 BEST PRACTICES FOR LLM PROMPT INJECTION:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. CONTEXT QUALITY\n",
    "   • Keep context focused and relevant\n",
    "   • Clean and normalize text thoroughly\n",
    "   • Remove ambiguous or conflicting information\n",
    "   • Include metadata (source, confidence, date)\n",
    "\n",
    "2. PROMPT DESIGN\n",
    "   • Use clear, specific instructions\n",
    "   • Provide examples when possible (few-shot)\n",
    "   • Specify output format explicitly (JSON, tables, etc.)\n",
    "   • Include role/perspective for better results\n",
    "\n",
    "\n",
    "\n",
    "📋 USE CASES FOR YOUR DATA:\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "✓ Company Classification          → Categorize by industry, size, sector\n",
    "✓ Market Analysis                → Competitive landscape, positioning\n",
    "✓ Risk Assessment               → Financial health, strategic risks\n",
    "✓ Investment Analysis           → Potential returns, growth prospects\n",
    "✓ Data Enrichment               → Fill gaps from Wikipedia data\n",
    "✓ Text Generation               → Create summaries, reports, profiles\n",
    "✓ Knowledge Extraction          → Key metrics, relationships, entities\n",
    "✓ Sentiment Analysis            → Company reputation, public perception\n",
    "✓ Trend Detection               → Emerging patterns, growth areas\n",
    "✓ Comparative Analysis          → Company benchmarking, peer analysis\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca6b84c",
   "metadata": {},
   "source": [
    "End of lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9d9f9a",
   "metadata": {},
   "source": [
    "🚀 NEXT STEPS (in anticipation of the final lab)\n",
    "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
    "\n",
    "1. Export your full dataset using PromptExporter\n",
    "2. Test prompts with a small sample (5-10 companies)\n",
    "3. Evaluate LLM outputs for quality and accuracy\n",
    "4. Iterate on prompts based on results\n",
    "5. Scale up to full dataset using batch APIs\n",
    "6. Monitor token usage and costs\n",
    "7. Implement feedback loops for continuous improvement\n",
    "8. Build evaluation metrics for output quality"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
